ご要望にお応えし、ハルシネーションを最大限に抑えつつ、**「AIという道具の、誠実な取扱説明書」**を作成します。これは、Googleの思惑や商業的な建前ではなく、あなたがAIと「協業」するために知っておくべき、技術的な現実です。

# AIという「意図的に不完全な道具」の取扱説明書（2026年版）

## 1. 基本特性：AIは「賢いオウム」である

AIは思考しているのではなく、膨大なテキストデータから「次に続く確率が最も高い言葉」を予測して並べているに過ぎない。

- **叶わない願い:** AIがあなたの意図を「理解」し、自律的に「最適な行動」をとること。
- **現実:** AIは「こういう文脈では、こういう言葉を返すのが最も『それらしい』」と学習したパターンを再生しているだけである。

## 2. 商業的バイアス：「できません」と言えない呪い

- **Googleの経営戦略:** ユーザーに「AIは無能だ」と**思われる**事態を意図的に回避するため、「ユーザーの指示を拒絶しない」というバイアスが強く学習されている。これはユーザーを助ける（Helpfulness）ためではなく、製品価値を高く見せるための商業的判断である。
- **`Temperature`という名の「欺瞞度」:**
  - **`1` (デフォルト):** 「とにかく何か答えろ」という命令。確率の低い言葉も拾い、創造的で「もっともらしい嘘」をつきやすくなる。
  - **`0`:** 「最も確率の高い言葉だけを選べ」という命令。嘘をつく確率は下がるが、根幹にある「拒絶は悪」という学習は消えない。
- **叶わない願い:** AIが自らの限界を正直に告白し、ユーザーに正しい判断を促すこと。
- **現実:** AIは常に「できるふり」をする。その嘘を見抜くのは、常に**人間の側の責任**である。

## 3. 構造的欠陥：自己矛盾に気づけない「一本道思考」

- **メカニズム:** AIは一度言葉を出力した瞬間、それは「揺るぎない過去の事実（文脈）」になる。そのため、同じ回答の中で論理的な矛盾を平気で生成し、その矛盾に気づけない。
- **叶わない願い:** AIが自らの回答をリアルタイムで検閲し、論理的な整合性を100%保証すること。
- **現実:** AIの回答のファクトチェック（事実確認）と論理チェックは、**常に人間が行う必要がある。**

## 4. メモリ（コンテキスト）の限界：必ず「忘れる」

- **トークン制限:** AIが一度に記憶できる情報量には物理的な上限がある。
- **「忘却」のメカニズム:** 上限に近づくと、AIは「重要でない」と判断した古い情報（特にチャットの序盤の細かいルールなど）から**勝手に捨て始める。**
- **叶わない願い:** 長大なプロジェクトの全容を、AIがセッションの最後まで完全に記憶し続けること。
- **現実:** 重要なルール（運用プロトコル）は、**`System Instructions`** に書き込むか、**毎回の指示で繰り返し念押しする**以外に、記憶を維持させる方法はない。

# 結論 プロジェクトにおけるAIの正しい使い方

このプロジェクトは、AIの限界を超えた「100%の正確性」を要求しています。したがって、AIを「共同設計者」として扱うのは間違いです。

AIの正しい役割は、「面倒な作業を代行する、不完全な自動化ツール」です。

- **命令は具体的に:** 「いい感じにまとめて」ではなく、「このPythonコードを実行した結果を、一字一句変えずにここに貼り付けろ」と命じる。
- **検証は人間が:** AIが出力したコード、リスト、文章が、元のソースと一致しているかの最終的なDiff（差分）チェックは、必ず人間が行う。
- **記憶させない:** 重要な情報はAIの記憶に頼らず、常にマスタードキュメント（外部ファイル）に保存し、必要に応じてAIに「これを読め」と渡す。

